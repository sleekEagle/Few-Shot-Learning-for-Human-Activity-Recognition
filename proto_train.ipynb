{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets\n",
    "from torch import optim as optim\n",
    "import torch.autograd as autograd\n",
    "import importlib\n",
    "import Extract_data\n",
    "importlib.reload(Extract_data)\n",
    "import Inference\n",
    "importlib.reload(Inference)\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import sys\n",
    "from os import walk\n",
    "import random\n",
    "import FCNN_serial\n",
    "importlib.reload(FCNN_serial)\n",
    "from scipy import signal\n",
    "import prototypical_loss\n",
    "importlib.reload(prototypical_loss)\n",
    "\n",
    "# torch.cuda.is_available() checks and returns a Boolean True if a GPU is available, else it'll return False\n",
    "is_cuda = torch.cuda.is_available()\n",
    "# If we have a GPU available, we'll set our device to GPU. We'll use this device variable later in our code.\n",
    "if is_cuda:\n",
    "    device = torch.device(\"cuda\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "device=torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Evaluate\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "11"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_support=6\n",
    "n_query=5\n",
    "n_samples=n_support+n_query\n",
    "n_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df,seen_classes,unseen_classes,sr=Extract_data.prepare_UTWENTE_data()\n",
    "participants=data_df['participant'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_df,seen_classes,unseen_classes,sr=Extract_data.prepare_UTWENTE_data()\n",
    "participants=data_df['participant'].unique()\n",
    "\n",
    "source_data,target_data=Extract_data.get_seen_unseen_data(data_df,seen_classes,unseen_classes,['1'],['1'])\n",
    "\n",
    "bs=60\n",
    "num_features=128\n",
    "in_channels=data_df['data'].iloc[0].shape[0]\n",
    "num_classes=len(np.unique(source_data['label']))\n",
    "lr=0.001\n",
    "discount=0.6\n",
    "segment_len_s=1\n",
    "window_len=sr*segment_len_s\n",
    "overlap=int(window_len*0.5)\n",
    "LAMBDA=0.0001\n",
    "num_selected=5\n",
    "kernel1_size=2\n",
    "kernel2_size=1\n",
    "num_skip=20\n",
    "\n",
    "model=FCNN_serial.HARmodel(num_classes=num_classes,in_channels=in_channels,\n",
    "                                    num_features=num_features,\n",
    "                                    kernel1_size=kernel1_size,\n",
    "                                    kernel2_size=kernel2_size)\n",
    "model=model.to(device)\n",
    "criterion=nn.CrossEntropyLoss()\n",
    "optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "windowed_df=Extract_data.get_windowed_df(data_df,window_len,overlap)\n",
    "participant1=participant2=participants[0]\n",
    "source_df,target_df=Extract_data.get_seen_unseen_data(windowed_df,seen_classes,unseen_classes,[participant1],[participant2])\n",
    "\n",
    "min_num=np.min(source_df.groupby('label').count()['data'].values)\n",
    "train_df,test_df=Extract_data.divide_data_samples(source_df,num_samples=min_num-3)\n",
    "\n",
    "selected_df,other_df=Extract_data.select_random_classes_data(train_df,num_selected)\n",
    "\n",
    "selected_batch,_=Extract_data.divide_data_samples(selected_df,num_samples=n_samples)\n",
    "selected_data=torch.from_numpy(np.array([d for d in selected_batch['data']]))\n",
    "selected_labels=torch.from_numpy(np.array([d for d in selected_batch['label']]))\n",
    "selected_data=selected_data.float()  \n",
    "selected_data=selected_data.to(device)\n",
    "selected_labels=selected_labels.to(device)\n",
    "\n",
    "selected_embedding,selected_pred=model(selected_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss, acc=loss_fn(selected_embedding,target=selected_labels,n_support=n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([55])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_name</th>\n",
       "      <th>data</th>\n",
       "      <th>participant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "      <td>1787</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity_name  data  participant\n",
       "label                                  \n",
       "0               1787  1787         1787\n",
       "1               1787  1787         1787\n",
       "2               1787  1787         1787\n",
       "4               1787  1787         1787\n",
       "6               1787  1787         1787"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array(['walk', 'upstairs', 'type', 'drink', 'talk'], dtype=object),\n",
       " array(['stand', 'smoke', 'eat'], dtype=object))"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df['activity_name'].unique(),other_df['activity_name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def euclidean_dist(x, y):\n",
    "    '''\n",
    "    Compute euclidean distance between two tensors\n",
    "    '''\n",
    "    # x: N x D\n",
    "    # y: M x D\n",
    "    n = x.size(0)\n",
    "    m = y.size(0)\n",
    "    d = x.size(1)\n",
    "    if d != y.size(1):\n",
    "        raise Exception\n",
    "\n",
    "    x = x.unsqueeze(1).expand(n, m, d)\n",
    "    y = y.unsqueeze(0).expand(n, m, d)\n",
    "\n",
    "    return torch.pow(x - y, 2).sum(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "target=selected_labels\n",
    "input=selected_embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6, 5)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cpu = target.to('cpu')\n",
    "input_cpu = input.to('cpu')\n",
    "\n",
    "def supp_idxs(c):\n",
    "        # FIXME when torch will support where as np\n",
    "        return target_cpu.eq(c).nonzero()[:n_support].squeeze(1)\n",
    "classes = torch.unique(target_cpu)\n",
    "n_classes = len(classes)\n",
    "n_query = target_cpu.eq(classes[0].item()).sum().item() - n_support\n",
    "n_support,n_query"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3, 5, 6, 7])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 5, 5,\n",
       "        5, 5, 5, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7,\n",
       "        7, 7, 7, 7, 7, 7, 7])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "support_idxs = list(map(supp_idxs, classes))\n",
    "prototypes = torch.stack([input_cpu[idx_list].mean(0) for idx_list in support_idxs])\n",
    "query_idxs = torch.stack(list(map(lambda c: target_cpu.eq(c).nonzero()[n_support:], classes))).view(-1)\n",
    "query_samples = input_cpu[query_idxs]\n",
    "dists = euclidean_dist(query_samples, prototypes)\n",
    "log_p_y = F.log_softmax(-dists, dim=1).view(n_classes, n_query, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([5, 5, 5])"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_p_y.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[15.8068, 21.4587, 21.0877, 24.9164, 19.2297],\n",
       "         [16.7870, 55.2084, 53.1533, 61.0909, 50.9115],\n",
       "         [18.3518, 52.4062, 50.5727, 48.2380, 50.5143],\n",
       "         [23.2521, 52.3595, 52.1024, 56.9914, 50.2366],\n",
       "         [15.5227, 33.5283, 33.4234, 33.6671, 31.4605],\n",
       "         [62.6541,  1.0825,  1.3130,  0.9753,  2.3998],\n",
       "         [67.7161,  2.9946,  2.5883,  2.1215,  4.3938],\n",
       "         [54.8156,  0.2482,  1.1325,  2.7638,  0.6256],\n",
       "         [63.3501,  1.0148,  1.6496,  1.4973,  2.1381],\n",
       "         [65.3004,  1.5265,  1.8015,  1.4714,  2.7855],\n",
       "         [46.0608,  1.2502,  1.0670,  1.9801,  1.3890],\n",
       "         [31.3672, 13.4647, 12.1627, 17.2334, 11.1742],\n",
       "         [29.0774,  9.9363,  7.9122, 10.4043,  8.5683],\n",
       "         [52.3547,  1.1000,  1.0389,  2.0864,  0.8667],\n",
       "         [36.7222,  4.9042,  4.2629,  8.0896,  3.7071],\n",
       "         [61.7008,  1.7683,  1.9037,  1.9942,  3.1957],\n",
       "         [55.9874,  3.3078,  2.9891,  5.7719,  3.2949],\n",
       "         [62.9060, 10.5471,  8.3844,  9.2086, 11.5929],\n",
       "         [67.9356,  2.7311,  2.8494,  1.7593,  4.5954],\n",
       "         [67.7160,  2.9473,  2.7182,  1.7270,  4.8519],\n",
       "         [52.8850,  2.7561,  1.8336,  1.6493,  3.0876],\n",
       "         [58.2273,  1.9717,  1.1570,  1.5091,  2.5906],\n",
       "         [64.7975,  3.4956,  2.1020,  1.9406,  4.9537],\n",
       "         [62.5526,  1.0047,  1.5571,  1.6538,  2.0921],\n",
       "         [43.2638,  3.4897,  3.1597,  5.6912,  2.2684]], grad_fn=<SumBackward1>),\n",
       " torch.Size([25, 5]))"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dists,dists.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>activity_name</th>\n",
       "      <th>participant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       data  activity_name  participant\n",
       "label                                  \n",
       "0         3              3            3\n",
       "1         3              3            3\n",
       "2         3              3            3\n",
       "3         5              5            5\n",
       "4         5              5            5\n",
       "5         5              5            5\n",
       "6         7              7            7\n",
       "7         5              5            5"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.eval()\n",
    "data=torch.from_numpy(np.array([d for d in train_df['data']]))\n",
    "labels=torch.from_numpy(np.array([d for d in train_df['label']]))\n",
    "data=data.float()  \n",
    "data=data.to(device)\n",
    "labels=labels.to(device)\n",
    "test_acc=Extract_data.get_acc(model,data,labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_prototypes(labels,embedding):\n",
    "    def get_indices(c):\n",
    "        return labels.eq(c).nonzero().squeeze(1)\n",
    "\n",
    "    classes = torch.unique(labels)\n",
    "    idxs = list(map(get_indices, classes))\n",
    "    prototypes = torch.stack([embedding[idx_list].mean(0) for idx_list in idxs])\n",
    "    \n",
    "    return prototypes,idxs,classes\n",
    "\n",
    "def euclidean_dist(x,y):\n",
    "    return torch.pow(x-y,2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_acc(model,data,labels,pred_index=1):\n",
    "    ret=model(data)\n",
    "    pred=ret[pred_index]\n",
    "    predmax=torch.max(pred,1)[1]\n",
    "    iscorrect=(predmax==labels)\n",
    "    num_correct=torch.sum(iscorrect).item()\n",
    "    num_total=iscorrect.shape[0]\n",
    "    acc=num_correct/num_total\n",
    "    return acc\n",
    "\n",
    "optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*discount\n",
    "model=model.eval()\n",
    "data=torch.from_numpy(np.array([d for d in test_df['data']]))\n",
    "labels=torch.from_numpy(np.array([d for d in test_df['label']]))\n",
    "data=data.float()  \n",
    "data=data.to(device)\n",
    "labels=labels.to(device)\n",
    "get_acc(model,data,labels,pred_index=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_batch,_=Extract_data.divide_data_samples(selected_df,num_samples=n_samples)\n",
    "selected_data=torch.from_numpy(np.array([d for d in selected_batch['data']]))\n",
    "selected_labels=torch.from_numpy(np.array([d for d in selected_batch['label']]))\n",
    "selected_data=selected_data.float()  \n",
    "selected_data=selected_data.to(device)\n",
    "selected_labels=selected_labels.to(device)\n",
    "\n",
    "selected_embedding,selected_pred=model(selected_data)\n",
    "\n",
    "loss, acc,t=prototypical_loss.prototypical_loss(selected_embedding,target=selected_labels,n_support=n_support)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([55, 128]), torch.Size([55]))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_embedding.shape,selected_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([36, 128]), torch.Size([36]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding,_=model(data)\n",
    "embedding.shape,labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=torch.from_numpy(np.array([d for d in test_df['data']]))\n",
    "labels=torch.from_numpy(np.array([d for d in test_df['label']]))\n",
    "data=data.float()  \n",
    "data=data.to(device)\n",
    "train_labels=labels.to(device)\n",
    "train_embedding,_=model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1.0,0.3333333432674408,1.0,1.0,1.0,0.6666666865348816,0.3333333432674408,0.6666666865348816,'"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n_support=10\n",
    "selected_df=train_df.groupby('label').head(n_support)\n",
    "selected_df['from']='train'\n",
    "\n",
    "min_num=np.min(test_df.groupby('label').count()['data'].values)\n",
    "df=test_df.groupby('label').head(min_num)\n",
    "total_df=selected_df.append(df)\n",
    "\n",
    "data=torch.from_numpy(np.array([d for d in total_df['data']]))\n",
    "labels=torch.from_numpy(np.array([d for d in total_df['label']]))\n",
    "data=data.float()  \n",
    "data=data.to(device)\n",
    "labels=labels.to(device)\n",
    "embedding,_=model(data)\n",
    "\n",
    "loss, acc, t=prototypical_loss.prototypical_loss(embedding,target=labels,n_support=n_support)\n",
    "per_class_acc=torch.mean(t.float(),dim=1)\n",
    "\n",
    "source_acc=[str(acc.item()) for acc in per_class_acc]\n",
    "source_acc=','.join(source_acc)+','\n",
    "source_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_name</th>\n",
       "      <th>data</th>\n",
       "      <th>participant</th>\n",
       "      <th>from</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       activity_name  data  participant  from\n",
       "label                                        \n",
       "0                 10    10           10    10\n",
       "1                 10    10           10    10\n",
       "2                 10    10           10    10\n",
       "3                 10    10           10    10\n",
       "4                 10    10           10    10\n",
       "5                 10    10           10    10\n",
       "6                 10    10           10    10\n",
       "7                 10    10           10    10"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selected_df.groupby('label').count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['from']='test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_df=selected_df.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>activity_name</th>\n",
       "      <th>data</th>\n",
       "      <th>from</th>\n",
       "      <th>label</th>\n",
       "      <th>participant</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1154</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[5.5069, 6.7566, 7.822, 8.0047, 8.0361, 7.967...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1100</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[6.9098, 6.8604, 6.3656, 6.3132, 6.1153, 4.94...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>828</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[-0.32388, 0.029639, 0.21350999999999998, 0.4...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>316</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[3.9958, 4.6419, 5.3732, 5.9856, 6.1365, 5.93...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1406</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[4.9094, 4.5718, 4.4677, 4.695, 4.6732, 4.167...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1587</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[4.8129, 4.1121, 4.3335, 4.5836, 4.4844, 3.06...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1467</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[-4.2818, -4.6368, -4.9924, -4.8207, -4.6677,...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[-1.7091, -1.2767, -0.60736, -0.25776, -0.084...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>710</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[0.42508, 1.0843, 1.568, 1.9041, 2.5528, 2.68...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1537</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[2.3542, 1.3295, 0.5518, -0.44569, -0.8194799...</td>\n",
       "      <td>train</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[6.6987, 6.6559, 6.2306, 6.156000000000001, 6...</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>738</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[-0.42268, -0.87644, -1.5521, -1.9298, -2.096...</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>walk</td>\n",
       "      <td>[[-3.4688, -3.6436, -3.4942, -3.3993, -3.1777,...</td>\n",
       "      <td>test</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     activity_name                                               data   from  \\\n",
       "1154          walk  [[5.5069, 6.7566, 7.822, 8.0047, 8.0361, 7.967...  train   \n",
       "1100          walk  [[6.9098, 6.8604, 6.3656, 6.3132, 6.1153, 4.94...  train   \n",
       "828           walk  [[-0.32388, 0.029639, 0.21350999999999998, 0.4...  train   \n",
       "316           walk  [[3.9958, 4.6419, 5.3732, 5.9856, 6.1365, 5.93...  train   \n",
       "1406          walk  [[4.9094, 4.5718, 4.4677, 4.695, 4.6732, 4.167...  train   \n",
       "1587          walk  [[4.8129, 4.1121, 4.3335, 4.5836, 4.4844, 3.06...  train   \n",
       "1467          walk  [[-4.2818, -4.6368, -4.9924, -4.8207, -4.6677,...  train   \n",
       "186           walk  [[-1.7091, -1.2767, -0.60736, -0.25776, -0.084...  train   \n",
       "710           walk  [[0.42508, 1.0843, 1.568, 1.9041, 2.5528, 2.68...  train   \n",
       "1537          walk  [[2.3542, 1.3295, 0.5518, -0.44569, -0.8194799...  train   \n",
       "22            walk  [[6.6987, 6.6559, 6.2306, 6.156000000000001, 6...   test   \n",
       "738           walk  [[-0.42268, -0.87644, -1.5521, -1.9298, -2.096...   test   \n",
       "1006          walk  [[-3.4688, -3.6436, -3.4942, -3.3993, -3.1777,...   test   \n",
       "\n",
       "     label participant  \n",
       "1154     0           1  \n",
       "1100     0           1  \n",
       "828      0           1  \n",
       "316      0           1  \n",
       "1406     0           1  \n",
       "1587     0           1  \n",
       "1467     0           1  \n",
       "186      0           1  \n",
       "710      0           1  \n",
       "1537     0           1  \n",
       "22       0           1  \n",
       "738      0           1  \n",
       "1006     0           1  "
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total_df[total_df['label']==0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*discount\n",
    "model=model.eval()\n",
    "\n",
    "df=test_df.groupby('label').head(3)\n",
    "data=torch.from_numpy(np.array([d for d in df['data']]))\n",
    "labels=torch.from_numpy(np.array([d for d in df['label']]))\n",
    "data=data.float()  \n",
    "data=data.to(device)\n",
    "labels=labels.to(device)\n",
    "embedding,_=model(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.1250)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss, acc,_=prototypical_loss.prototypical_loss(embedding,target=labels,n_support=2)\n",
    "acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>activity_name</th>\n",
       "      <th>participant</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>label</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "      <td>1790</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1792</td>\n",
       "      <td>1792</td>\n",
       "      <td>1792</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       data  activity_name  participant\n",
       "label                                  \n",
       "0      1790           1790         1790\n",
       "1      1790           1790         1790\n",
       "2      1790           1790         1790\n",
       "3      1790           1790         1790\n",
       "4      1792           1792         1792"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_df.groupby('label').head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Look here!!\n",
    "'''\n",
    "\n",
    "target_df=target_df.sample(n=len(target_df))\n",
    "min_num=np.min(target_df.groupby('label').count()['data'].values)\n",
    "df=target_df.groupby('label').head(min_num)\n",
    "data=torch.from_numpy(np.array([d for d in df['data']]))\n",
    "labels=torch.from_numpy(np.array([d for d in df['label']]))\n",
    "data=data.float()  \n",
    "data=data.to(device)\n",
    "labels=labels.to(device)\n",
    "\n",
    "FSL_embeddings,_=best_model(data)\n",
    "\n",
    "loss, acc, t=prototypical_loss.prototypical_loss(FSL_embeddings,target=labels,n_support=10)\n",
    "per_class_acc=torch.mean(t.float(),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.625"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "test_acc.item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-9003059be78f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     69\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 71\u001b[0;31m         \u001b[0mselected_batch\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mExtract_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdivide_data_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_samples\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m         \u001b[0mselected_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'data'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0mselected_labels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0md\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0md\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mselected_batch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'label'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/qumulo/qhome/lnw8px/models/HAR_FSL/Extract_data.py\u001b[0m in \u001b[0;36mdivide_data_samples\u001b[0;34m(data_df, num_samples)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mtotal_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0mtrain_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m     \u001b[0mtest_idx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetdiff1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_idx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_idx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m     \u001b[0mtest_df\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest_idx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_df\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_df\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36msetdiff1d\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36msetdiff1d\u001b[0;34m(ar1, ar2, assume_unique)\u001b[0m\n\u001b[1;32m    783\u001b[0m         \u001b[0mar1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mravel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    784\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 785\u001b[0;31m         \u001b[0mar1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    786\u001b[0m         \u001b[0mar2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    787\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mar1\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0min1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mar2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0massume_unique\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minvert\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<__array_function__ internals>\u001b[0m in \u001b[0;36munique\u001b[0;34m(*args, **kwargs)\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36munique\u001b[0;34m(ar, return_index, return_inverse, return_counts, axis)\u001b[0m\n\u001b[1;32m    261\u001b[0m     \u001b[0mar\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    262\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0maxis\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 263\u001b[0;31m         \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unique1d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mar\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_inverse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_counts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    264\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0m_unpack_tuple\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/numpy/lib/arraysetops.py\u001b[0m in \u001b[0;36m_unique1d\u001b[0;34m(ar, return_index, return_inverse, return_counts)\u001b[0m\n\u001b[1;32m    309\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mperm\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0mar\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0maux\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m     \u001b[0mmask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mempty\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maux\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbool_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_df,seen_classes,unseen_classes,sr=Extract_data.prepare_UTWENTE_data()\n",
    "participants=data_df['participant'].unique()\n",
    "\n",
    "source_data,target_data=Extract_data.get_seen_unseen_data(data_df,seen_classes,unseen_classes,['1'],['1'])\n",
    "\n",
    "bs=60\n",
    "num_features=128\n",
    "in_channels=data_df['data'].iloc[0].shape[0]\n",
    "num_classes=len(np.unique(source_data['label']))\n",
    "lr=0.001\n",
    "discount=0.6\n",
    "segment_len_s=1\n",
    "window_len=sr*segment_len_s\n",
    "overlap=int(window_len*0.5)\n",
    "num_selected=5\n",
    "kernel1_size=2\n",
    "kernel2_size=1\n",
    "n_support=6\n",
    "n_query=5\n",
    "n_samples=n_support+n_query\n",
    "\n",
    "file='/home/lnw8px/models/HAR_FSL/UTWENTE/proto/test.txt'\n",
    "f = open(file, \"a\") \n",
    "source_classes=source_data.groupby('label').head(1).sort_values(by=['label'])['activity_name'].values\n",
    "target_classes=target_data.groupby('label').head(1).sort_values(by=['label'])['activity_name'].values\n",
    "\n",
    "source_classes=','.join(source_classes)\n",
    "target_classes=','.join(target_classes)\n",
    "\n",
    "towrite='bs,discount,kernel1_size,kernel2_size,participant1,participant2,test_acc,n_shots,'\n",
    "towrite+='fsl_acc \\n'\n",
    "f.write(towrite)\n",
    "f.close()\n",
    "#print(towrite)\n",
    "\n",
    "#get windowed data\n",
    "windowed_df=Extract_data.get_windowed_df(data_df,window_len,overlap)\n",
    "participant1=participant2=participants[0]\n",
    "\n",
    "for i in range(1000):\n",
    "    #for participant1 in participants:\n",
    "        #for participant2 in participants:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    source_df,target_df=Extract_data.get_seen_unseen_data(windowed_df,seen_classes,unseen_classes,[participant1],[participant2])\n",
    "\n",
    "    min_num=np.min(source_df.groupby('label').count()['data'].values)\n",
    "    train_df,test_df=Extract_data.divide_data_samples(source_df,num_samples=min_num-3)\n",
    "\n",
    "    model=FCNN_serial.HARmodel(num_classes=num_classes,in_channels=in_channels,\n",
    "                        num_features=num_features,\n",
    "                        kernel1_size=kernel1_size,\n",
    "                        kernel2_size=kernel2_size)\n",
    "    model=model.to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_list,center_loss_list=[],[]\n",
    "    max_acc=0\n",
    "    best_model=0\n",
    "    for i in range(1000):\n",
    "        selected_df,_=Extract_data.select_random_classes_data(train_df,num_selected)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        selected_batch,_=Extract_data.divide_data_samples(selected_df,num_samples=n_samples)\n",
    "        selected_data=torch.from_numpy(np.array([d for d in selected_batch['data']]))\n",
    "        selected_labels=torch.from_numpy(np.array([d for d in selected_batch['label']]))\n",
    "        selected_data=selected_data.float()  \n",
    "        selected_data=selected_data.to(device)\n",
    "        selected_labels=selected_labels.to(device)\n",
    "\n",
    "        selected_embedding,selected_pred=model(selected_data)\n",
    "\n",
    "        loss, acc,_=prototypical_loss.prototypical_loss(selected_embedding,target=selected_labels,n_support=n_support)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #train_loss_list.append(label_loss.item())\n",
    "        #center_loss_list.append(loss_center.item())\n",
    "\n",
    "        if((i>0) and (i%100==0)):\n",
    "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*discount\n",
    "            model=model.eval()\n",
    "            \n",
    "            #get per class accuracy on test source data\n",
    "            n_support=10\n",
    "            selected_df=train_df.groupby('label').head(n_support)\n",
    "            selected_df['from']='train'\n",
    "\n",
    "            min_num=np.min(test_df.groupby('label').count()['data'].values)\n",
    "            df=test_df.groupby('label').head(min_num)\n",
    "            total_df=selected_df.append(df)\n",
    "\n",
    "            data=torch.from_numpy(np.array([d for d in total_df['data']]))\n",
    "            labels=torch.from_numpy(np.array([d for d in total_df['label']]))\n",
    "            data=data.float()  \n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            embedding,_=model(data)\n",
    "\n",
    "            loss, test_acc, t=prototypical_loss.prototypical_loss(embedding,target=labels,n_support=n_support)\n",
    "            test_acc=test_acc.item()\n",
    "            per_class_source_acc=torch.mean(t.float(),dim=1)\n",
    "            \n",
    "            model=model.train()\n",
    "            if(test_acc>max_acc):\n",
    "                max_acc=test_acc\n",
    "                best_model=model\n",
    "\n",
    "    source_acc=[str(acc.item()) for acc in per_class_source_acc]\n",
    "    source_acc=','.join(source_acc)+','\n",
    "\n",
    "    target_df=target_df.sample(n=len(target_df))\n",
    "    for n_shots in range(1,6):\n",
    "        min_num=np.min(target_df.groupby('label').count()['data'].values)\n",
    "        df=target_df.groupby('label').head(min_num)\n",
    "        data=torch.from_numpy(np.array([d for d in df['data']]))\n",
    "        labels=torch.from_numpy(np.array([d for d in df['label']]))\n",
    "        data=data.float()  \n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        FSL_embeddings,_=best_model(data)\n",
    "\n",
    "        loss, acc, t=prototypical_loss.prototypical_loss(FSL_embeddings,target=labels,n_support=n_shots)\n",
    "        per_class_acc=torch.mean(t.float(),dim=1)\n",
    "        \n",
    "        towrite=str(bs)+','+str(discount)+','+str(kernel1_size)+','+str(kernel2_size)+','\n",
    "        towrite+=str(participant1)+','+str(participant2)+','\n",
    "        towrite+=str(max_acc)+','\n",
    "        towrite+=str(n_shots)+','\n",
    "        towrite+=str(acc.item())+'\\n'\n",
    "\n",
    "        f = open(file, \"a\")\n",
    "        f.write(towrite)\n",
    "        f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-f4c9d33fe47b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     78\u001b[0m         \u001b[0mselected_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mselected_pred\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0m_\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprototypical_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprototypical_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mselected_embedding\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mselected_labels\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mn_support\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mn_support\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/sfs/qumulo/qhome/lnw8px/models/HAR_FSL/prototypical_loss.py\u001b[0m in \u001b[0;36mprototypical_loss\u001b[0;34m(input, target, n_support)\u001b[0m\n\u001b[1;32m     82\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mlog_p_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgather\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_inds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlog_p_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m     \u001b[0macc_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0my_hat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meq\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_inds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqueeze\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_df,seen_classes,unseen_classes,sr=Extract_data.prepare_UTWENTE_data()\n",
    "participants=data_df['participant'].unique()\n",
    "\n",
    "source_data,target_data=Extract_data.get_seen_unseen_data(data_df,seen_classes,unseen_classes,['1'],['1'])\n",
    "\n",
    "bs=60\n",
    "num_features=128\n",
    "in_channels=data_df['data'].iloc[0].shape[0]\n",
    "num_classes=len(np.unique(source_data['label']))\n",
    "lr=0.001\n",
    "discount=0.6\n",
    "segment_len_s=1\n",
    "window_len=sr*segment_len_s\n",
    "overlap=int(window_len*0.5)\n",
    "num_selected=5\n",
    "kernel1_size=2\n",
    "kernel2_size=1\n",
    "n_support=6\n",
    "n_query=5\n",
    "n_samples=n_support+n_query\n",
    "\n",
    "file='/home/lnw8px/models/HAR_FSL/UTWENTE/proto/test.txt'\n",
    "f = open(file, \"a\") \n",
    "source_classes=source_data.groupby('label').head(1).sort_values(by=['label'])['activity_name'].values\n",
    "target_classes=target_data.groupby('label').head(1).sort_values(by=['label'])['activity_name'].values\n",
    "\n",
    "source_classes=','.join(source_classes)\n",
    "target_classes=','.join(target_classes)\n",
    "\n",
    "towrite='bs,discount,kernel1_size,kernel2_size,participant1,participant2,test_acc,n_shots,'\n",
    "towrite+='fsl_acc'\n",
    "f.write(towrite)\n",
    "f.close()\n",
    "#print(towrite)\n",
    "\n",
    "#get windowed data\n",
    "windowed_df=Extract_data.get_windowed_df(data_df,window_len,overlap)\n",
    "participant1=participant2=participants[0]\n",
    "\n",
    "for i in range(1000):\n",
    "    #for participant1 in participants:\n",
    "        #for participant2 in participants:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    source_df,target_df=Extract_data.get_seen_unseen_data(windowed_df,seen_classes,unseen_classes,[participant1],[participant2])\n",
    "\n",
    "    min_num=np.min(source_df.groupby('label').count()['data'].values)\n",
    "    train_df,test_df=Extract_data.divide_data_samples(source_df,num_samples=min_num-3)\n",
    "\n",
    "    model=FCNN_serial.HARmodel(num_classes=num_classes,in_channels=in_channels,\n",
    "                        num_features=num_features,\n",
    "                        kernel1_size=kernel1_size,\n",
    "                        kernel2_size=kernel2_size)\n",
    "    model=model.to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_list,center_loss_list=[],[]\n",
    "    max_acc=0\n",
    "    best_model=0\n",
    "    for i in range(1000):\n",
    "        selected_df,_=Extract_data.select_random_classes_data(train_df,num_selected)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        selected_batch,_=Extract_data.divide_data_samples(selected_df,num_samples=n_samples)\n",
    "        selected_data=torch.from_numpy(np.array([d for d in selected_batch['data']]))\n",
    "        selected_labels=torch.from_numpy(np.array([d for d in selected_batch['label']]))\n",
    "        selected_data=selected_data.float()  \n",
    "        selected_data=selected_data.to(device)\n",
    "        selected_labels=selected_labels.to(device)\n",
    "\n",
    "        selected_embedding,selected_pred=model(selected_data)\n",
    "\n",
    "        loss, acc,_=prototypical_loss.prototypical_loss(selected_embedding,target=selected_labels,n_support=n_support)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #train_loss_list.append(label_loss.item())\n",
    "        #center_loss_list.append(loss_center.item())\n",
    "\n",
    "        if((i>0) and (i%100==0)):\n",
    "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*discount\n",
    "            model=model.eval()\n",
    "            data=torch.from_numpy(np.array([d for d in test_df['data']]))\n",
    "            labels=torch.from_numpy(np.array([d for d in test_df['label']]))\n",
    "            data=data.float()  \n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            test_acc=Extract_data.get_acc(model,data,labels)\n",
    "            model=model.train()\n",
    "            if(test_acc>max_acc):\n",
    "                max_acc=test_acc\n",
    "                best_model=model\n",
    "\n",
    "    best_model=best_model.eval()\n",
    "    #get per class accuracy on test source data\n",
    "    n_support=10\n",
    "    selected_df=train_df.groupby('label').head(n_support)\n",
    "    selected_df['from']='train'\n",
    "\n",
    "    min_num=np.min(test_df.groupby('label').count()['data'].values)\n",
    "    df=test_df.groupby('label').head(min_num)\n",
    "    total_df=selected_df.append(df)\n",
    "\n",
    "    data=torch.from_numpy(np.array([d for d in total_df['data']]))\n",
    "    labels=torch.from_numpy(np.array([d for d in total_df['label']]))\n",
    "    data=data.float()  \n",
    "    data=data.to(device)\n",
    "    labels=labels.to(device)\n",
    "    embedding,_=model(data)\n",
    "\n",
    "    loss, acc, t=prototypical_loss.prototypical_loss(embedding,target=labels,n_support=n_support)\n",
    "    per_class_acc=torch.mean(t.float(),dim=1)\n",
    "\n",
    "    source_acc=[str(acc.item()) for acc in per_class_acc]\n",
    "    source_acc=','.join(source_acc)+','\n",
    "    source_acc\n",
    "\n",
    "    target_df=target_df.sample(n=len(target_df))\n",
    "    for n_shots in range(1,6):\n",
    "        min_num=np.min(target_df.groupby('label').count()['data'].values)\n",
    "        df=target_df.groupby('label').head(min_num)\n",
    "        data=torch.from_numpy(np.array([d for d in df['data']]))\n",
    "        labels=torch.from_numpy(np.array([d for d in df['label']]))\n",
    "        data=data.float()  \n",
    "        data=data.to(device)\n",
    "        labels=labels.to(device)\n",
    "\n",
    "        FSL_embeddings,_=best_model(data)\n",
    "\n",
    "        loss, acc, t=prototypical_loss.prototypical_loss(FSL_embeddings,target=labels,n_support=n_shots)\n",
    "        per_class_acc=torch.mean(t.float(),dim=1)\n",
    "        \n",
    "        towrite=str(bs)+','+str(discount)+','+str(kernel1_size)+','+str(kernel2_size)+','\n",
    "        towrite+=str(participant1)+','+str(participant2)+','\n",
    "        towrite+=str(max_acc)+','\n",
    "        towrite+=str(n_shots)+','\n",
    "        towrite+=str(acc)\n",
    "\n",
    "        f = open(file, \"a\")\n",
    "        f.write(towrite)\n",
    "        f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "pd.set_option('mode.chained_assignment', None)\n",
    "torch.cuda.empty_cache()\n",
    "\n",
    "data_df,seen_classes,unseen_classes,sr=Extract_data.prepare_UTWENTE_data()\n",
    "participants=data_df['participant'].unique()\n",
    "\n",
    "source_data,target_data=Extract_data.get_seen_unseen_data(data_df,seen_classes,unseen_classes,['1'],['1'])\n",
    "\n",
    "bs=60\n",
    "num_features=128\n",
    "in_channels=data_df['data'].iloc[0].shape[0]\n",
    "num_classes=len(np.unique(source_data['label']))\n",
    "lr=0.001\n",
    "discount=0.6\n",
    "segment_len_s=1\n",
    "window_len=sr*segment_len_s\n",
    "overlap=int(window_len*0.5)\n",
    "num_selected=5\n",
    "kernel1_size=2\n",
    "kernel2_size=1\n",
    "n_support=6\n",
    "n_query=5\n",
    "n_samples=n_support+n_query\n",
    "\n",
    "file='/home/lnw8px/models/HAR_FSL/test.txt'\n",
    "f = open(file, \"a\") \n",
    "source_classes=source_data.groupby('label').head(1).sort_values(by=['label'])['activity_name'].values\n",
    "target_classes=target_data.groupby('label').head(1).sort_values(by=['label'])['activity_name'].values\n",
    "\n",
    "source_classes=','.join(source_classes)\n",
    "target_classes=','.join(target_classes)\n",
    "\n",
    "towrite='bs,discount,kernel1_size,kernel2_size,participant1,participant2,test_acc,n_shots,'\n",
    "towrite+='sim_acc,svm_acc,knn_total_acc,'\n",
    "towrite+=source_classes+','\n",
    "towrite+=','.join(['sim '+item for item in target_classes.split(',')])+','\n",
    "towrite+=','.join(['svm '+item for item in target_classes.split(',')])+','\n",
    "towrite+=','.join(['knn '+item for item in target_classes.split(',')])+','\n",
    "towrite+='len_source,len_target,sim_source,sim_target,svm_source,svm_target,knn_source,knn_target\\n'\n",
    "f.write(towrite)\n",
    "f.close()\n",
    "#print(towrite)\n",
    "\n",
    "#get windowed data\n",
    "windowed_df=Extract_data.get_windowed_df(data_df,window_len,overlap)\n",
    "participant1=participant2=participants[0]\n",
    "\n",
    "for i in range(1):\n",
    "    #for participant1 in participants:\n",
    "        #for participant2 in participants:\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "    source_df,target_df=Extract_data.get_seen_unseen_data(windowed_df,seen_classes,unseen_classes,[participant1],[participant2])\n",
    "\n",
    "    min_num=np.min(source_df.groupby('label').count()['data'].values)\n",
    "    train_df,test_df=Extract_data.divide_data_samples(source_df,num_samples=min_num-3)\n",
    "    #train_df,test_df=Extract_data.divide_data_ratio(source_data_df,0.8)\n",
    "\n",
    "    #calculate weights for classes\n",
    "    counts=train_df.groupby(['label']).count()['data'].values\n",
    "    weights=[1/item for item in counts]\n",
    "    weights=torch.tensor(weights/np.sum(weights))\n",
    "    weights=weights.float()\n",
    "    weights=weights.to(device)\n",
    "\n",
    "    model=FCNN_serial.HARmodel(num_classes=num_classes,in_channels=in_channels,\n",
    "                        num_features=num_features,\n",
    "                        kernel1_size=kernel1_size,\n",
    "                        kernel2_size=kernel2_size)\n",
    "    model=model.to(device)\n",
    "    optimizer=torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    train_loss_list,center_loss_list=[],[]\n",
    "    max_acc=0\n",
    "    best_model=0\n",
    "    for i in range(1000):\n",
    "        selected_df,_=Extract_data.select_random_classes_data(train_df,num_selected)\n",
    "\n",
    "        model.zero_grad()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        selected_batch,_=Extract_data.divide_data_samples(selected_df,num_samples=n_samples)\n",
    "        selected_data=torch.from_numpy(np.array([d for d in selected_batch['data']]))\n",
    "        selected_labels=torch.from_numpy(np.array([d for d in selected_batch['label']]))\n",
    "        selected_data=selected_data.float()  \n",
    "        selected_data=selected_data.to(device)\n",
    "        selected_labels=selected_labels.to(device)\n",
    "\n",
    "        selected_embedding,selected_pred=model(selected_data)\n",
    "\n",
    "        loss, acc,_=prototypical_loss.prototypical_loss(selected_embedding,target=selected_labels,n_support=n_support)\n",
    "\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        #train_loss_list.append(label_loss.item())\n",
    "        #center_loss_list.append(loss_center.item())\n",
    "\n",
    "        if((i>0) and (i%100==0)):\n",
    "            optimizer.param_groups[0]['lr']=optimizer.param_groups[0]['lr']*discount\n",
    "            model=model.eval()\n",
    "            data=torch.from_numpy(np.array([d for d in test_df['data']]))\n",
    "            labels=torch.from_numpy(np.array([d for d in test_df['label']]))\n",
    "            data=data.float()  \n",
    "            data=data.to(device)\n",
    "            labels=labels.to(device)\n",
    "            test_acc=Extract_data.get_acc(model,data,labels)\n",
    "            model=model.train()\n",
    "            if(test_acc>max_acc):\n",
    "                max_acc=test_acc\n",
    "                best_model=model\n",
    "\n",
    "    best_model=best_model.eval()\n",
    "    #get per class accuracy on test source data\n",
    "    n_support=10\n",
    "    selected_df=train_df.groupby('label').head(n_support)\n",
    "    selected_df['from']='train'\n",
    "\n",
    "    min_num=np.min(test_df.groupby('label').count()['data'].values)\n",
    "    df=test_df.groupby('label').head(min_num)\n",
    "    total_df=selected_df.append(df)\n",
    "\n",
    "    data=torch.from_numpy(np.array([d for d in total_df['data']]))\n",
    "    labels=torch.from_numpy(np.array([d for d in total_df['label']]))\n",
    "    data=data.float()  \n",
    "    data=data.to(device)\n",
    "    labels=labels.to(device)\n",
    "    embedding,_=model(data)\n",
    "\n",
    "    loss, acc, t=prototypical_loss.prototypical_loss(embedding,target=labels,n_support=n_support)\n",
    "    per_class_acc=torch.mean(t.float(),dim=1)\n",
    "\n",
    "    source_acc=[str(acc.item()) for acc in per_class_acc]\n",
    "    source_acc=','.join(source_acc)+','\n",
    "    source_acc\n",
    "\n",
    "    for j in range(1):\n",
    "        target_df=target_df.sample(n=len(target_df))\n",
    "        for n_shots in range(1,6):\n",
    "            #extract data from FSL\n",
    "            FSL_train_df=target_df.groupby(['label']).head(n_shots)\n",
    "            train_indices=FSL_train_df.index\n",
    "            FSL_test_df=target_df[~target_df.index.isin(train_indices)]\n",
    "\n",
    "            #extract same number of samples from both labels\n",
    "            min_samples=min(FSL_test_df.groupby('label').count().iloc[:,0].values)\n",
    "            FSL_test_df=FSL_test_df.groupby('label').head(min_samples)\n",
    "\n",
    "            FSL_train_data=torch.from_numpy(np.array([d for d in FSL_train_df['data']]))\n",
    "            FSL_train_labels=torch.from_numpy(np.array([d for d in FSL_train_df['label']]))\n",
    "            FSL_train_data=FSL_train_data.float()  \n",
    "\n",
    "            FSL_test_data=torch.from_numpy(np.array([d for d in FSL_test_df['data']]))\n",
    "            FSL_test_labels=torch.from_numpy(np.array([d for d in FSL_test_df['label']]))\n",
    "            FSL_test_data=FSL_test_data.float()  \n",
    "\n",
    "            FSL_train_data=FSL_train_data.to(device)\n",
    "            FSL_test_data=FSL_test_data.to(device)\n",
    "\n",
    "            FSL_train_embeddings,_=best_model(FSL_train_data)\n",
    "            FSL_test_embeddings,_=best_model(FSL_test_data)\n",
    "\n",
    "            FSL_train_df=pd.DataFrame()\n",
    "            FSL_train_df['data']=list(FSL_train_embeddings.cpu().detach().numpy())\n",
    "            FSL_train_df['label']=list(FSL_train_labels.cpu().detach().numpy())\n",
    "\n",
    "            FSL_test_df=pd.DataFrame()\n",
    "            FSL_test_df['data']=list(FSL_test_embeddings.cpu().detach().numpy())\n",
    "            FSL_test_df['label']=list(FSL_test_labels.cpu().detach().numpy())\n",
    "\n",
    "            sim_total_acc,sim_per_class_acc=Inference.train_similarity_classifier(FSL_train_df.copy(deep=True),FSL_test_df.copy(deep=True))\n",
    "            svm_total_acc,svm_per_class_acc=Inference.train_SVM(FSL_train_df.copy(deep=True),FSL_test_df.copy(deep=True))\n",
    "            knn_total_acc,_,_,knn_per_class_acc=Inference.train_KNN(FSL_train_df.copy(deep=True),FSL_test_df.copy(deep=True))\n",
    "            towrite=str(bs)+','+str(discount)+','+str(kernel1_size)+','+str(kernel2_size)+','\n",
    "            towrite+=str(participant1)+','+str(participant2)+','\n",
    "            towrite+=str(max_acc)+','\n",
    "            towrite+=str(n_shots)+','\n",
    "            towrite+=str(sim_total_acc)+','\n",
    "            towrite+=str(svm_total_acc)+','\n",
    "            towrite+=str(knn_total_acc)+','\n",
    "            towrite+=source_acc\n",
    "            sim_per_class_acc=list(sim_per_class_acc['iscorrect'].values)\n",
    "            sim_per_class_acc=','.join([str(item) for item in sim_per_class_acc])\n",
    "            svm_per_class_acc=list(svm_per_class_acc['iscorrect'].values)\n",
    "            svm_per_class_acc=','.join([str(item) for item in svm_per_class_acc])\n",
    "            knn_per_class_acc=list(knn_per_class_acc['iscorrect'].values)\n",
    "            knn_per_class_acc=','.join([str(item) for item in knn_per_class_acc])\n",
    "\n",
    "            towrite+=sim_per_class_acc+','\n",
    "            towrite+=svm_per_class_acc+','\n",
    "            towrite+=knn_per_class_acc+','\n",
    "\n",
    "            #classify source vs target data\n",
    "            test_df_cp=test_df.copy(deep=True)\n",
    "            target_df_cp=target_df[['data','label']].copy(deep=True)\n",
    "            test_df_cp['source']=1\n",
    "            target_df_cp['source']=0\n",
    "            total_df=pd.DataFrame()\n",
    "            total_df=target_df_cp.append(test_df_cp)\n",
    "            selected_train_df=total_df.groupby(['label','source']).head(n_shots)\n",
    "            train_indices=selected_train_df.index\n",
    "            selected_test_df=total_df[~total_df.index.isin(train_indices)]\n",
    "            selected_train_df.groupby(['label','source']).count(),selected_test_df.groupby(['label','source']).count()\n",
    "            selected_train_df['label']=selected_train_df['source']\n",
    "            selected_test_df['label']=selected_test_df['source']\n",
    "\n",
    "            selected_train_data=torch.from_numpy(np.array([d for d in selected_train_df['data']]))\n",
    "            selected_train_labels=np.array([d for d in selected_train_df['label']])\n",
    "            selected_train_data=selected_train_data.float()  \n",
    "            selected_train_data=selected_train_data.to(device)\n",
    "            selected_train_embedding,_=model(selected_train_data)\n",
    "            selected_train_embedding=selected_train_embedding.cpu().detach().numpy()\n",
    "\n",
    "            selected_test_data=torch.from_numpy(np.array([d for d in selected_test_df['data']]))\n",
    "            selected_test_labels=np.array([d for d in selected_test_df['label']])\n",
    "            selected_test_data=selected_test_data.float()  \n",
    "            selected_test_data=selected_test_data.to(device)\n",
    "            selected_test_embedding,_=model(selected_test_data)\n",
    "            selected_test_embedding=selected_test_embedding.cpu().detach().numpy()\n",
    "\n",
    "            emb_train_df=pd.DataFrame()\n",
    "            emb_train_df['data']=list(selected_train_embedding)\n",
    "            emb_train_df['label']=selected_train_labels\n",
    "\n",
    "            emb_test_df=pd.DataFrame()\n",
    "            emb_test_df['data']=list(selected_test_embedding)\n",
    "            emb_test_df['label']=selected_test_labels\n",
    "\n",
    "            #extract same number of samples from both labels\n",
    "            min_samples=min(emb_test_df.groupby('label').count().values)[0]\n",
    "            emb_test_df=emb_test_df.groupby('label').head(min_samples)\n",
    "\n",
    "            sim_total_acc,sim_per_class_acc=Inference.train_similarity_classifier(emb_train_df.copy(deep=True),emb_test_df.copy(deep=True))\n",
    "            knn_total_acc,pred,test_labels,knn_per_class_acc=Inference.train_KNN(emb_train_df.copy(deep=True),emb_test_df.copy(deep=True))\n",
    "            svm_total_acc,svm_per_class_acc=Inference.train_SVM(emb_train_df.copy(deep=True),emb_test_df.copy(deep=True))\n",
    "\n",
    "            counts=emb_test_df.groupby('label').count()['data'].values\n",
    "            towrite+=str(counts[0])+','+str(counts[1])+','\n",
    "            sim_per_class_acc=list(sim_per_class_acc['iscorrect'].values)\n",
    "            knn_per_class_acc=list(knn_per_class_acc['iscorrect'].values)\n",
    "            svm_per_class_acc=list(svm_per_class_acc['iscorrect'].values)\n",
    "            towrite+=str(sim_per_class_acc[0])+','+str(sim_per_class_acc[1])+','\n",
    "            towrite+=str(svm_per_class_acc[0])+','+str(svm_per_class_acc[1])+','\n",
    "            towrite+=str(knn_per_class_acc[0])+','+str(knn_per_class_acc[1])+'\\n'\n",
    "\n",
    "            f = open(file, \"a\")\n",
    "            f.write(towrite)\n",
    "            f.close() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyTorch 1.5.1 Py3.7",
   "language": "python",
   "name": "pytorch-1.5.1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
